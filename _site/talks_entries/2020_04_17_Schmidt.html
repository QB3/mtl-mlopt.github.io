<!DOCTYPE html>




<html
 dir="ltr"
 lang="en"
 class="has-navbar-fixed-top">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content=#ffffff>
    <link rel="stylesheet" href="/assets/css/app.css">
    <link rel="shortcut icon" type="image/png"
           href="/favicon.png" 
    />
    <script defer src="https://unpkg.com/alpinejs@3.9.0/dist/cdn.min.js"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css">
    
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma-social@1/bin/bulma-social.min.css">
    
    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Faster Algorithms for Deep Learning? | Montreal MLOpt</title>
<meta name="generator" content="Jekyll v3.9.3" />
<meta property="og:title" content="Faster Algorithms for Deep Learning?" />
<meta name="author" content="Mark Schmidt" />
<meta property="og:locale" content="en" />
<meta name="description" content="Recent works have shown that stochastic gradient descent (SGD) achieves the fast convergence rates of full-batch gradient descent for over-parameterized models satisfying certain interpolation conditions. However, the step-size used in these works depends on unknown quantities and SGD’s practical performance heavily relies on the choice of this step-size. We propose to use line-search techniques to automatically set the step-size when training models that can interpolate the data. In the interpolation setting, we prove that SGD with a stochastic variant of the classic Armijo line-search attains the deterministic convergence rates for both convex and strongly-convex functions. Under additional assumptions, SGD with Armijo line-search is shown to achieve fast convergence for non-convex functions. Furthermore, we show that stochastic extra-gradient with a Lipschitz line-search attains linear convergence for an important class of non-convex functions and saddle-point problems satisfying interpolation. To improve the proposed methods’ practical performance, we give heuristics to use larger step-sizes and acceleration. We compare the proposed algorithms against numerous optimization methods on standard classification tasks using both kernel methods and deep networks. The proposed methods result in competitive performance across all models and datasets, while being robust to the precise choices of hyper-parameters. For multi-class classification using deep networks, SGD with Armijo line-search results in both faster convergence and better generalization." />
<meta property="og:description" content="Recent works have shown that stochastic gradient descent (SGD) achieves the fast convergence rates of full-batch gradient descent for over-parameterized models satisfying certain interpolation conditions. However, the step-size used in these works depends on unknown quantities and SGD’s practical performance heavily relies on the choice of this step-size. We propose to use line-search techniques to automatically set the step-size when training models that can interpolate the data. In the interpolation setting, we prove that SGD with a stochastic variant of the classic Armijo line-search attains the deterministic convergence rates for both convex and strongly-convex functions. Under additional assumptions, SGD with Armijo line-search is shown to achieve fast convergence for non-convex functions. Furthermore, we show that stochastic extra-gradient with a Lipschitz line-search attains linear convergence for an important class of non-convex functions and saddle-point problems satisfying interpolation. To improve the proposed methods’ practical performance, we give heuristics to use larger step-sizes and acceleration. We compare the proposed algorithms against numerous optimization methods on standard classification tasks using both kernel methods and deep networks. The proposed methods result in competitive performance across all models and datasets, while being robust to the precise choices of hyper-parameters. For multi-class classification using deep networks, SGD with Armijo line-search results in both faster convergence and better generalization." />
<link rel="canonical" href="http://localhost:4000/talks_entries/2020_04_17_Schmidt.html" />
<meta property="og:url" content="http://localhost:4000/talks_entries/2020_04_17_Schmidt.html" />
<meta property="og:site_name" content="Montreal MLOpt" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-04-17T16:00:00-04:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Faster Algorithms for Deep Learning?" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Mark Schmidt"},"dateModified":"2020-04-17T16:00:00-04:00","datePublished":"2020-04-17T16:00:00-04:00","description":"Recent works have shown that stochastic gradient descent (SGD) achieves the fast convergence rates of full-batch gradient descent for over-parameterized models satisfying certain interpolation conditions. However, the step-size used in these works depends on unknown quantities and SGD’s practical performance heavily relies on the choice of this step-size. We propose to use line-search techniques to automatically set the step-size when training models that can interpolate the data. In the interpolation setting, we prove that SGD with a stochastic variant of the classic Armijo line-search attains the deterministic convergence rates for both convex and strongly-convex functions. Under additional assumptions, SGD with Armijo line-search is shown to achieve fast convergence for non-convex functions. Furthermore, we show that stochastic extra-gradient with a Lipschitz line-search attains linear convergence for an important class of non-convex functions and saddle-point problems satisfying interpolation. To improve the proposed methods’ practical performance, we give heuristics to use larger step-sizes and acceleration. We compare the proposed algorithms against numerous optimization methods on standard classification tasks using both kernel methods and deep networks. The proposed methods result in competitive performance across all models and datasets, while being robust to the precise choices of hyper-parameters. For multi-class classification using deep networks, SGD with Armijo line-search results in both faster convergence and better generalization.","headline":"Faster Algorithms for Deep Learning?","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/talks_entries/2020_04_17_Schmidt.html"},"url":"http://localhost:4000/talks_entries/2020_04_17_Schmidt.html"}</script>
<!-- End Jekyll SEO tag -->

    <!-- head scripts --></head>

  <body>
    <nav class="navbar is-primary  is-fixed-top " x-data="{ openNav: false }">
    <div class="container">
        <div class="navbar-brand">
            <a href="/" class="navbar-item">
                Montreal MLOpt
            </a>
            <a role="button" class="navbar-burger burger" aria-label="menu" aria-expanded="false" data-target="navMenu" :class="{ 'is-active': openNav }" x-on:click="openNav = !openNav">
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
            </a>
        </div>
        <div class="navbar-menu" id="navMenu" :class="{ 'is-active': openNav }">
            <div class="navbar-start">
                <a href="/" class="navbar-item ">Home</a>
                
                
                    
                    <a href="/talks" class="navbar-item ">Talks schedule</a>
                    
                
                    
                    <div class="navbar-item has-dropdown is-hoverable ">
                        <a href="/organizers" class="navbar-link ">Organizers</a>
                        <div class="navbar-dropdown">
                            
                            <a href="/organizers" class="navbar-item ">All</a>
                            
                            <a href="/organizers#current" class="navbar-item ">Current</a>
                            
                            <a href="/organizers#alumnis" class="navbar-item ">Alumnis</a>
                            
                        </div>
                    </div>
                    
                
                    
                    <a href="/archive/index.html" class="navbar-item ">Archive</a>
                    
                
                
            </div>

            <div class="navbar-end">
                
            </div>

        </div>
    </div>
</nav>

    
        <section class="hero  is-small  is-bold is-primary" >
    <div class="hero-body ">
        <div class="container">
            <h1 class="title is-2">Faster Algorithms for Deep Learning?</h1>
            <p class="subtitle is-3"></p>
            
        </div>
    </div>
</section>
    
    


    <section class="section">
        <div class="container">
            <div class="columns is-multiline">
                
                <div class="column is-12">
                    
                    

                    
                    
                    
<div class="content">
    <h2> Speaker: Mark Schmidt </h2>

<div> <b>Where:</b> Virtual.</div> 
<div> <b>When:</b> April 17, 2020 at 16:00.</div>


<figure class="image is-16by9"> 
    <iframe class="has-ratio" 
        src="https://www.youtube.com/embed/nk4M-kYvaNU?start=0&showinfo=0" 
        frameborder="0" 
        allowfullscreen
        >
    </iframe>
</figure>




<h3> Reference(s) </h3>
<ul>

<li> https://papers.nips.cc/paper/2019/hash/2557911c1bf75c2b643afb4ecbfc8ec2-Abstract.html </li>

</ul>


<h2> Abstract </h2>

<div> <p>Recent works have shown that stochastic gradient descent (SGD) achieves the fast convergence rates of full-batch gradient descent for over-parameterized models satisfying certain interpolation conditions. However, the step-size used in these works depends on unknown quantities and SGD’s practical performance heavily relies on the choice of this step-size. We propose to use line-search techniques to automatically set the step-size when training models that can interpolate the data. In the interpolation setting, we prove that SGD with a stochastic variant of the classic Armijo line-search attains the deterministic convergence rates for both convex and strongly-convex functions. Under additional assumptions, SGD with Armijo line-search is shown to achieve fast convergence for non-convex functions. Furthermore, we show that stochastic extra-gradient with a Lipschitz line-search attains linear convergence for an important class of non-convex functions and saddle-point problems satisfying interpolation. To improve the proposed methods’ practical performance, we give heuristics to use larger step-sizes and acceleration. We compare the proposed algorithms against numerous optimization methods on standard classification tasks using both kernel methods and deep networks. The proposed methods result in competitive performance across all models and datasets, while being robust to the precise choices of hyper-parameters. For multi-class classification using deep networks, SGD with Armijo line-search results in both faster convergence and better generalization.</p>
</div>


<div class="tags">
    
</div>


<p><strong>Share</strong></p>
<div class="buttons ">
    <a class="button is-medium is-facebook"
       onclick="window.open('https://www.facebook.com/share.php?u=http://localhost:4000/talks_entries/2020_04_17_Schmidt.html');">
        <span class="icon"><i class="fab fa-facebook fa-lg"></i></span>
    </a>
    <a class="button is-medium is-twitter"
       onclick="window.open('https://twitter.com/intent/tweet?text=http://localhost:4000/talks_entries/2020_04_17_Schmidt.html');">
        <span class="icon"><i class="fab fa-twitter fa-lg"></i></span>
    </a>
    <a class="button is-medium is-linkedin"
       onclick="window.open('https://www.linkedin.com/shareArticle?mini=true&url=http://localhost:4000/talks_entries/2020_04_17_Schmidt.html&title=Faster+Algorithms+for+Deep+Learning%3F&summary=&source=');">
        <span class="icon"><i class="fab fa-linkedin fa-lg"></i></span>
    </a>
    <a class="button is-medium is-reddit"
       onclick="window.open('https://reddit.com/submit?url=http://localhost:4000/talks_entries/2020_04_17_Schmidt.html');">
        <span class="icon"><i class="fab fa-reddit fa-lg"></i></span>
    </a>
</div>



</div>
                </div>
                
            </div>
        </div>
    </section>
    
        <footer class="footer">
    <div class="container">
        
        

        <div class="content is-small has-text-centered">
            <p class="">Theme built by <a href="https://www.csrhymes.com">C.S. Rhymes</a></p>
        </div>
    </div>
</footer>

    
    <script src="/assets/js/app.js" type="text/javascript"></script><!-- footer scripts --></body>
</html>
