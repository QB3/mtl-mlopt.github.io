<!DOCTYPE html>




<html
 dir="ltr"
 lang="en"
 class="has-navbar-fixed-top">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content=#ffffff>
    <link rel="stylesheet" href="/assets/css/app.css">
    <link rel="shortcut icon" type="image/png"
           href="/favicon.png" 
    />
    <script defer src="https://unpkg.com/alpinejs@3.9.0/dist/cdn.min.js"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css">
    
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma-social@1/bin/bulma-social.min.css">
    
    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>alpha-ReQ : Assessing Representation Quality in Self-Supervised Learning by measuring eigenspectrum decay | Montreal MLOpt</title>
<meta name="generator" content="Jekyll v3.9.3" />
<meta property="og:title" content="alpha-ReQ : Assessing Representation Quality in Self-Supervised Learning by measuring eigenspectrum decay" />
<meta name="author" content="Arna Gosh" />
<meta property="og:locale" content="en" />
<meta name="description" content="Self-Supervised Learning (SSL) with large-scale unlabelled datasets enables learning useful representations for multiple downstream tasks. However, assessing the quality of such representations efficiently poses nontrivial challenges. Existing approaches train linear probes (with frozen features) to evaluate performance on a given task. This is expensive both computationally, since it requires retraining a new prediction head for each downstream task, and statistically, requires task-specific labels for multiple tasks. This poses a natural question, how do we efficiently determine the “goodness” of representations learned with SSL across a wide range of potential downstream tasks? In particular, a task-agnostic statistical measure of representation quality, that predicts generalization without explicit downstream task evaluation, would be highly desirable. In this work, we analyze characteristics of learned representations f_\theta, in well-trained neural networks with canonical architectures &amp; across SSL objectives. We observe that the eigenspectrum of the empirical feature covariance Cov(f_\theta) can be well approximated with the family of power-law distribution. We analytically and empirically (using multiple datasets, e.g. CIFAR, STL10, MIT67, ImageNet) demonstrate that the decay coefficient alpha serves as a measure of representation quality for tasks that are solvable with a linear readout, i.e. there exist well-defined intervals for alpha where models exhibit excellent downstream generalization. Furthermore, our experiments suggest that key design parameters in SSL algorithms, such as BarlowTwins, implicitly modulate the decay coefficient of the eigenspectrum (alpha). As alpha depends only on the features themselves, this measure for model selection with hyperparameter tuning for BarlowTwins enables search with less compute." />
<meta property="og:description" content="Self-Supervised Learning (SSL) with large-scale unlabelled datasets enables learning useful representations for multiple downstream tasks. However, assessing the quality of such representations efficiently poses nontrivial challenges. Existing approaches train linear probes (with frozen features) to evaluate performance on a given task. This is expensive both computationally, since it requires retraining a new prediction head for each downstream task, and statistically, requires task-specific labels for multiple tasks. This poses a natural question, how do we efficiently determine the “goodness” of representations learned with SSL across a wide range of potential downstream tasks? In particular, a task-agnostic statistical measure of representation quality, that predicts generalization without explicit downstream task evaluation, would be highly desirable. In this work, we analyze characteristics of learned representations f_\theta, in well-trained neural networks with canonical architectures &amp; across SSL objectives. We observe that the eigenspectrum of the empirical feature covariance Cov(f_\theta) can be well approximated with the family of power-law distribution. We analytically and empirically (using multiple datasets, e.g. CIFAR, STL10, MIT67, ImageNet) demonstrate that the decay coefficient alpha serves as a measure of representation quality for tasks that are solvable with a linear readout, i.e. there exist well-defined intervals for alpha where models exhibit excellent downstream generalization. Furthermore, our experiments suggest that key design parameters in SSL algorithms, such as BarlowTwins, implicitly modulate the decay coefficient of the eigenspectrum (alpha). As alpha depends only on the features themselves, this measure for model selection with hyperparameter tuning for BarlowTwins enables search with less compute." />
<link rel="canonical" href="http://localhost:4000/talks_entries/2023/2023_04_19_Gosh.html" />
<meta property="og:url" content="http://localhost:4000/talks_entries/2023/2023_04_19_Gosh.html" />
<meta property="og:site_name" content="Montreal MLOpt" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-04-19T13:30:00-04:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="alpha-ReQ : Assessing Representation Quality in Self-Supervised Learning by measuring eigenspectrum decay" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Arna Gosh"},"dateModified":"2023-04-19T13:30:00-04:00","datePublished":"2023-04-19T13:30:00-04:00","description":"Self-Supervised Learning (SSL) with large-scale unlabelled datasets enables learning useful representations for multiple downstream tasks. However, assessing the quality of such representations efficiently poses nontrivial challenges. Existing approaches train linear probes (with frozen features) to evaluate performance on a given task. This is expensive both computationally, since it requires retraining a new prediction head for each downstream task, and statistically, requires task-specific labels for multiple tasks. This poses a natural question, how do we efficiently determine the “goodness” of representations learned with SSL across a wide range of potential downstream tasks? In particular, a task-agnostic statistical measure of representation quality, that predicts generalization without explicit downstream task evaluation, would be highly desirable. In this work, we analyze characteristics of learned representations f_\\theta, in well-trained neural networks with canonical architectures &amp; across SSL objectives. We observe that the eigenspectrum of the empirical feature covariance Cov(f_\\theta) can be well approximated with the family of power-law distribution. We analytically and empirically (using multiple datasets, e.g. CIFAR, STL10, MIT67, ImageNet) demonstrate that the decay coefficient alpha serves as a measure of representation quality for tasks that are solvable with a linear readout, i.e. there exist well-defined intervals for alpha where models exhibit excellent downstream generalization. Furthermore, our experiments suggest that key design parameters in SSL algorithms, such as BarlowTwins, implicitly modulate the decay coefficient of the eigenspectrum (alpha). As alpha depends only on the features themselves, this measure for model selection with hyperparameter tuning for BarlowTwins enables search with less compute.","headline":"alpha-ReQ : Assessing Representation Quality in Self-Supervised Learning by measuring eigenspectrum decay","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/talks_entries/2023/2023_04_19_Gosh.html"},"url":"http://localhost:4000/talks_entries/2023/2023_04_19_Gosh.html"}</script>
<!-- End Jekyll SEO tag -->

    <!-- head scripts --></head>

  <body>
    <nav class="navbar is-primary  is-fixed-top " x-data="{ openNav: false }">
    <div class="container">
        <div class="navbar-brand">
            <a href="/" class="navbar-item">
                Montreal MLOpt
            </a>
            <a role="button" class="navbar-burger burger" aria-label="menu" aria-expanded="false" data-target="navMenu" :class="{ 'is-active': openNav }" x-on:click="openNav = !openNav">
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
            </a>
        </div>
        <div class="navbar-menu" id="navMenu" :class="{ 'is-active': openNav }">
            <div class="navbar-start">
                <a href="/" class="navbar-item ">Home</a>
                
                
                    
                    <a href="/talks" class="navbar-item ">Talks schedule</a>
                    
                
                    
                    <div class="navbar-item has-dropdown is-hoverable ">
                        <a href="/organizers" class="navbar-link ">Organizers</a>
                        <div class="navbar-dropdown">
                            
                            <a href="/organizers" class="navbar-item ">All</a>
                            
                            <a href="/organizers#current" class="navbar-item ">Current</a>
                            
                            <a href="/organizers#alumnis" class="navbar-item ">Past organizers</a>
                            
                        </div>
                    </div>
                    
                
                    
                    <a href="/archive/index.html" class="navbar-item ">Archive</a>
                    
                
                
            </div>

            <div class="navbar-end">
                
            </div>

        </div>
    </div>
</nav>

    
        <section class="hero  is-small  is-bold is-primary" >
    <div class="hero-body ">
        <div class="container">
            <h1 class="title is-2">alpha-ReQ : Assessing Representation Quality in Self-Supervised Learning by measuring eigenspectrum decay</h1>
            <p class="subtitle is-3"></p>
            
        </div>
    </div>
</section>
    
    


    <section class="section">
        <div class="container">
            <div class="columns is-multiline">
                
                <div class="column is-12">
                    
                    

                    
                    
                    
<div class="content">
    <h2> Speaker: Arna Gosh </h2>

<div> <b>Where:</b> Mila, Agora.</div> 
<div> <b>When:</b> April 19, 2023 at 13:30.</div>








<h2> Abstract </h2>

<div> <p>Self-Supervised Learning (SSL) with large-scale unlabelled datasets enables learning useful representations for multiple downstream tasks. However, assessing the quality of such representations efficiently poses nontrivial challenges. Existing approaches train linear probes (with frozen features) to evaluate performance on a given task. This is expensive both computationally, since it requires retraining a new prediction head for each downstream task, and statistically, requires task-specific labels for multiple tasks. This poses a natural question, how do we efficiently determine the “goodness” of representations learned with SSL across a wide range of potential downstream tasks? In particular, a task-agnostic statistical measure of representation quality, that predicts generalization without explicit downstream task evaluation, would be highly desirable. In this work, we analyze characteristics of learned representations f_\theta, in well-trained neural networks with canonical architectures &amp; across SSL objectives. We observe that the eigenspectrum of the empirical feature covariance Cov(f_\theta) can be well approximated with the family of power-law distribution. We analytically and empirically (using multiple datasets, e.g. CIFAR, STL10, MIT67, ImageNet) demonstrate that the decay coefficient alpha serves as a measure of representation quality for tasks that are solvable with a linear readout, i.e. there exist well-defined intervals for alpha where models exhibit excellent downstream generalization. Furthermore, our experiments suggest that key design parameters in SSL algorithms, such as BarlowTwins, implicitly modulate the decay coefficient of the eigenspectrum (alpha). As alpha depends only on the features themselves, this measure for model selection with hyperparameter tuning for BarlowTwins enables search with less compute.</p>
</div>


<div class="tags">
    
</div>


<p><strong>Share</strong></p>
<div class="buttons ">
    <a class="button is-medium is-facebook"
       onclick="window.open('https://www.facebook.com/share.php?u=http://localhost:4000/talks_entries/2023/2023_04_19_Gosh.html');">
        <span class="icon"><i class="fab fa-facebook fa-lg"></i></span>
    </a>
    <a class="button is-medium is-twitter"
       onclick="window.open('https://twitter.com/intent/tweet?text=http://localhost:4000/talks_entries/2023/2023_04_19_Gosh.html');">
        <span class="icon"><i class="fab fa-twitter fa-lg"></i></span>
    </a>
    <a class="button is-medium is-linkedin"
       onclick="window.open('https://www.linkedin.com/shareArticle?mini=true&url=http://localhost:4000/talks_entries/2023/2023_04_19_Gosh.html&title=alpha-ReQ+%3A+Assessing+Representation+Quality+in+Self-Supervised+Learning+by+measuring+eigenspectrum+decay&summary=&source=');">
        <span class="icon"><i class="fab fa-linkedin fa-lg"></i></span>
    </a>
    <a class="button is-medium is-reddit"
       onclick="window.open('https://reddit.com/submit?url=http://localhost:4000/talks_entries/2023/2023_04_19_Gosh.html');">
        <span class="icon"><i class="fab fa-reddit fa-lg"></i></span>
    </a>
</div>



</div>
                </div>
                
            </div>
        </div>
    </section>
    
        <footer class="footer">
    <div class="container">
        
        

        <div class="content is-small has-text-centered">
            <p class="">Theme built by <a href="https://www.csrhymes.com">C.S. Rhymes</a></p>
        </div>
    </div>
</footer>

    
    <script src="/assets/js/app.js" type="text/javascript"></script><!-- footer scripts --></body>
</html>
